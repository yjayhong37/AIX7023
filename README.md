# AIX 7023 (Natural Language Processing with Deep Learning, 2022-2)


Repository for AIX 7023, Department of Artificial Intelligence, Dongguk University

---------------------------------------------------------------------------------------------------------------------


# 1. Grading Policy

    1. Quiz (20%) : Quiz on ‘Suggested Readings’ is held every week during practical lecture.
                    
    2. Mid-term Exam (30%) :  8th week (Exact time will be announced separately)
    
    3. Project (40%) : Students work on team projects during the semester, replacing final grades.
                       Details will be explained in the first hour.
                        
    4.Participation (10%) : Interactions with professors during lectures and presentations and
                            contributions during practical time are evaluated.
                           
---------------------------------------------------------------------------------------------------------------------

# 2. Project Evaluation Policy

| Subject | Evaluation Items | Scores |
| ---------------- |---------------------------------------------------------------------------------------- | ---- |
| **Report writing** | 1. Is the project report well organized and written in an easy-to-understand manner ? | 15 |
|| 2.  Are citations appropriate ? | 10 |
| **Achievement** | 1. What is the contribution of the work ? | 10 |
|| 2. How much has it improved in terms of performance ? | 15 |
| **Originality** | 1. What is the originality in the research content and how much did it affect the performance improvement of the algorithm ? | 20 |
|| 2. Which part of the source codes are written by authors ? Are those codes are critical to the the performance improvement of the algorithm ? | 20 |
| **Presentation** | 1. Is the presentation well prepared ? | 5 |
|| 2. Does the presenter explain well simply and clearly? |  5|
|| **Total** | 100 |

---------------------------------------------------------------------------------------------------------------------

# 3. Course Work Plan

    1.  Weekly lectures consist of theory lectures and practical lectures.
   
    2.  Lecture slides are provided through Dongguk e-class site.
    
    3. Quiz on ‘Suggested Readings’ is held during each practical lecture.
    
    4. Weekly schedule is as following (JK: Jihie Kim, KK: Kwangil Kim)
    
    
   ## 3.1) Word Embeddings - JK
   
  Term Project Handout:
 
  ### (1) [Project Handout (Robust QA track)](http://web.stanford.edu/class/cs224n/project/default-final-project-handout-robustqa-track.pdf)
  
  ### (2) [Project Handout (IID SQuAD track)](https://web.stanford.edu/class/cs224n/project/default-final-project-handout-squad-track.pdf)
  
  ### (3) [End to End Question-Answering System Using NLP and SQuAD Dataset](https://www.analyticsvidhya.com/blog/2021/11/end-to-end-question-answering-system-using-nlp-and-squad-dataset/)
  
  Suggested Readings:
  
  ### (1) [Efficient Estimation of Word Representations in Vector Space (original word2vec paper)](http://arxiv.org/pdf/1301.3781.pdf)
  
  ### (2) [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  
  Additional Readings:
  
  ### (1) [GloVe: Global Vectors for Word Representation (original GloVe paper)](http://nlp.stanford.edu/pubs/glove.pdf)
  
  ### (2) [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)
  
  ### (3) [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)
  
  ### (4) [A Latent Variable Model Approach to PMI-based Word Embeddings](http://aclweb.org/anthology/Q16-1028)
  
  ### (5) [Linear Algebraic Structure of Word Senses, with Applications to Polysemy](https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320)
  
  ### (6) [On the Dimensionality of Word Embedding](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding.pdf)
  
  
  ---------------------------------------------------------------------------------------------------------------------
  
  
   ## 3.2) Neural Networks - KK
   
   Suggested Readings:
   
  ### (1)  [CS231n notes on network architectures](http://cs231n.github.io/neural-networks-1/)
  
  ### (2)  [CS231n notes on backprop](http://cs231n.github.io/optimization-2/)
  
  Additional Readings:
  
  ### (1)  [matrix calculus notes](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf)
  
  ### (2)  [Review of differential calculus](https://web.stanford.edu/class/cs224n/readings/review-differential-calculus.pdf)
  
  ### (3)  [Derivatives, Backpropagation, and Vectorization](http://cs231n.stanford.edu/handouts/derivatives.pdf)
  
  ### (4)  [Learning Representations by Backpropagating Errors (seminal Rumelhart et al. backpropagation paper)](http://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
  
  ### (5)  [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
  
  ### (6)  [Natural Language Processing (Almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
  
  ---------------------------------------------------------------------------------------------------------------------
  
  ## 3.3) Dependency Parsing - KK
  
Suggested Readings:

### (1)  [Jurafsky & Martin Chapter 14](https://web.stanford.edu/~jurafsky/slp3/14.pdf)

Additional Readings:

### (1)  [Incrementality in Deterministic Dependency Parsing](https://www.aclweb.org/anthology/W/W04/W04-0308.pdf)

### (2)  [A Fast and Accurate Dependency Parser using Neural Networks](https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf)

### (3)  [Globally Normalized Transition-Based Neural Networks](https://arxiv.org/pdf/1603.06042.pdf)

### (4)  [Universal Stanford Dependencies: A cross-linguistic typology](http://nlp.stanford.edu/~manning/papers/USD_LREC14_UD_revision.pdf)

### (5)  [Universal Dependencies website](http://universaldependencies.org/)

### (6)  [Dependency Parsing](http://www.morganclaypool.com/doi/abs/10.2200/S00169ED1V01Y200901HLT002)

---------------------------------------------------------------------------------------------------------------------

  ## 3.4) Recurrent Neural Networks - JK
  
Suggested Readings:

### (1)  [N-gram Language Models (textbook chapter)](https://web.stanford.edu/~jurafsky/slp3/3.pdf)

### (2)  [Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.1 and 10.2)](http://www.deeplearningbook.org/contents/rnn.html)

Additional Readings:

### (1)  [Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.3, 10.5, 10.7-10.12)](http://www.deeplearningbook.org/contents/rnn.html)

### (2)  [The Unreasonable Effectiveness of Recurrent Neural Networks (blog post overview)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

### (3)  [On Chomsky and the Two Cultures of Statistical Learning](http://norvig.com/chomsky.html)

### (4)  [Learning long-term dependencies with gradient descent is difficult (one of the original vanishing gradient papers)](http://ai.dinfo.unifi.it/paolo/ps/tnn-94-gradient.pdf)

### (5)  [On the difficulty of training Recurrent Neural Networks (proof of vanishing gradient problem)](https://arxiv.org/pdf/1211.5063.pdf)

### (6)  [Vanishing Gradients Jupyter Notebook (demo for feedforward networks)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html)

### (7)  [Understanding LSTM Networks (blog post overview)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

---------------------------------------------------------------------------------------------------------------------

  ## 3.5) Seq2Seq Model and Neural Machine Translation - JK
  
Suggested Readings:

### (1)  [Sequence to Sequence Learning with Neural Networks (original seq2seq NMT paper)](https://arxiv.org/pdf/1409.3215.pdf)

### (2)  [BLEU (original paper)](https://www.aclweb.org/anthology/P02-1040.pdf)

Additional Readings:

### (1)  [Statistical Machine Translation slides, CS224n 2015 (lectures 2/3/4)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/syllabus.shtml)

### (2)  [Statistical Machine Translation (book by Philipp Koehn)](https://www.cambridge.org/core/books/statistical-machine-translation/94EADF9F680558E13BE759997553CDE5)

### (3)  [Sequence Transduction with Recurrent Neural Networks (early seq2seq speech recognition paper)](https://arxiv.org/pdf/1211.3711.pdf)

---------------------------------------------------------------------------------------------------------------------

  ## 3.6) Attention Mechanism - JK
  
Suggested Readings:

### (1)  [Neural Machine Translation by Jointly Learning to Align and Translate (original seq2seq+attention paper)](https://arxiv.org/pdf/1409.0473.pdf)

### (2)  [Attention and Augmented Recurrent Neural Networks (blog post overview)](https://distill.pub/2016/augmented-rnns/)

Additional Readings:

### (1)  [Massive Exploration of Neural Machine Translation Architectures (practical advice for hyperparameter choices)](https://arxiv.org/pdf/1703.03906.pdf)

###(2)  [Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models](https://arxiv.org/abs/1604.00788.pdf)

### (3)  [Revisiting Character-Based Neural Machine Translation with Capacity and Compression](https://arxiv.org/pdf/1808.09943.pdf)

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.7) Transformer - JK

Suggested Readings:

### (1)  [Attention Is All You Need](https://arxiv.org/abs/1706.03762.pdf)

### (2)  [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

Additional Readings:

### (1)  [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

### (2)  [Transformer (Google AI blog post)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

### (3)  [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf)

### (4)  [Music Transformer: Generating music with long-term structure](https://arxiv.org/pdf/1809.04281.pdf)

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.8) Pretrained Language Models - JK

Suggested Readings:

(1)  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

(2)  [Contextual Word Representations: A Contextual Introduction](https://arxiv.org/abs/1902.06006.pdf)

Additional Readings:

(1)  [The Illustrated BERT, ELMo, and co.](http://jalammar.github.io/illustrated-bert/)

(2)  [Martin & Jurafsky Chapter on Transfer Learning](https://web.stanford.edu/~jurafsky/slp3/11.pdf)

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.9) Mid-term Exam - JK & KK
 
---------------------------------------------------------------------------------------------------------------------
 
  ## 3.10) Question Answering - JK
  
---------------------------------------------------------------------------------------------------------------------

Suggested Readings:

### (1)  [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf)

### (2)  [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/pdf/1502.05698))

Additional Readings:

### (1)  [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/pdf/1606.05250.pdf)

### (2)  [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/pdf/1906.00300.pdf)

### (3)  [Learning Dense Representations of Phrases at Scale](https://arxiv.org/pdf/2012.12624.pdf)

### (4)  [Bidirectional Attention Flow for Machine Comprehension](https://arxiv.org/pdf/1611.01603.pdf)

### (5)  [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/pdf/1704.00051.pdf)

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.11) Representing and Using Knowledge in NLP - JK
  
Suggested readings:

### (1)  [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)

### (2)  [Language Models as Knowledge Bases?](https://www.aclweb.org/anthology/D19-1250.pdf)

Additional Readings:

### (1)  [Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling](https://arxiv.org/pdf/1906.07241.pdf)

### (2)  [Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model](https://arxiv.org/pdf/1912.09637.pdf)

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.12) Chatbots and Dialog System - KK
**TBD**

---------------------------------------------------------------------------------------------------------------------
 
  ## 3.13) Natural Language Generation – KK
  
Suggested readings:

### (1)  [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751.pdf)

### (2)  [How NOT To Evaluate Your Dialogue System](https://arxiv.org/abs/1603.08023.pdf)

Additional Readings:

### (1)  [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368.pdf)

### (2)  [Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833.pdf)

---------------------------------------------------------------------------------------------------------------------

  ## 3.14) Information Extraction - JK

**TBD**

---------------------------------------------------------------------------------------------------------------------

  ## 3.15) Project Report & Evaluation - JK & KK

**TBD**

---------------------------------------------------------------------------------------------------------------------
  
  
### If anyone has any question regarding this course, Do not hesitate to contact T.A( Hong Yeongjun)

### yjayhong37@dgu.ac.kr 

### Students Slack Channel Might be open as soon as this fall semester begins
  
  
  






